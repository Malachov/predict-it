#!/usr/bin/python

# %%
""" This is main module for making predictions.

It contain functions

- predict() - More return types - Depends on config
- predict_multiple() - Predict multiple columns at once
- compare_models() - Test on data that was not in test set and compare models errors

Examples:
=========

    predictions = predictit.predict(np.random.randn(100, 1), predicts=3, showplot=1)

Do not edit this file if you are user, it's not necessary! Only call function from here. The only file to edit is
configuration.py. If you are developer, edit as you need.

There are working examples in main readme and also in test_it module. Particular modules functionality is visible in
visual.py in tests. """

from __future__ import annotations
from typing import cast, Any
import sys
from pathlib import Path
import os
import warnings
import argparse
import multiprocessing
import inspect
import io


# TODO delete me
import sys
from pathlib import Path

sys.path.insert(0, (Path.cwd().parent / "mypythontools").as_posix())
import mypythontools


import numpy as np
import pandas as pd
from tabulate import tabulate

import mydatapreprocessing as mdp
import mylogging
import mypythontools


# Lazy imports
# from tabulate import tabulate

# Get module path and insert in sys path for working even if opened from other cwd (current working directory)
try:
    predictit_root_path = Path(__file__).resolve().parents[1]
except (Exception,):
    # Jupyter notebook has no __file__, but inspect is much slower if no jupyter
    predictit_root_path = (
        Path(os.path.abspath(inspect.getframeinfo(inspect.currentframe()).filename)).resolve().parents[1]
    )

if predictit_root_path.as_posix() not in sys.path:
    sys.path.insert(0, predictit_root_path.as_posix())

import predictit
from predictit.configuration import config as config_default
from predictit.misc import GLOBAL_VARS


def predict(
    data=None,
    predicted_column: None | str | int = None,
    config: predictit.configuration.Config | dict | None = None,
    **kwargs,
) -> predictit.result_classes.Result:
    """Make predictions mostly on time-series data. Data input and other config options can be set up in
    configuration.py or overwritten on the fly. Setup can be also done as function input arguments or as command line
    arguments (it will overwrite config values).

    There are working examples in main readme and also in test_it module.

    Function can be configured from with config from configuration, with command line arguments as wel as with
    function parameters. There are only two possible positional parameters - `data` and `predicted_column`. Rest of
    parameters must be named parameters. Params are not documented here, because all config params works here in
    function passed as kwargs.

    Args:
        data (np.ndarray, pd.DataFrame, str): Time series. Can be 2-D - more columns. Can be numpy array, DataFrame,
            path to file or url.
            Examples: "/home/user/my.json", or "https://yoururl/your.csv" or np.random.randn(100, 2).
        predicted_column (None | str | int, optional): Index of predicted column or it's name (dataframe).
            If list with more values only the first one will be evaluated (use predict_multiple_columns function
            if you need that. Default to None.
        config (predictit.configuration.Config | dict | None, optional): Settings as Config instance or dictionary.
            Check class for what you can use. If None, then default config will be used. Defaults to None.
        **kwargs (dict, optional): There is much more parameters' of predict function. Check configuration.py
            for parameters details.

    Returns:
        predictit.result_classes.Result: Return best prediction {np.ndarray}, all models results {np.ndarray},
        detailed results{dict}, interactive plot, print tables of results and other results.

    """
    config = predictit._helpers.parse_config(config, config_default, kwargs)

    # Edit configuration.py default values with arguments values if exist
    if data is not None:
        config.data_input.data = data

    if predicted_column is not None:
        config.data_input.predicted_column = predicted_column

    predictit._helpers.logger_init_from_config(config.output.logger_subconfig)

    if not config.config_optimization.optimization:
        config.config_optimization.optimization = {"Not optimized": [True]}

    # Do not repeat actually mean evaluate once
    if not config.prediction.repeatit:
        config.prediction.repeatit = 1

    _GUI = GLOBAL_VARS.GUI

    # Add everything printed + warnings to variable to be able to print in GUI
    if _GUI:
        stdout = sys.stdout
        sys.stdout = io.StringIO()

    # Don't want to define in gui condition, so if not gui, do nothing
    if _GUI:

        def update_gui(content, html_id):
            try:
                predictit.gui_start.edit_gui_py(content, html_id)
            except (Exception,):
                pass

    else:

        def update_gui(content, html_id):
            pass

    ###############
    ### ANCHOR ### Data
    #############

    # Definition of the table for spent time on code parts
    time_table = mypythontools.misc.TimeTable()

    update_gui("Data loading and preprocessing", "progress_phase")

    data = mdp.load_data.load_data(
        config.data_input.data,
        header=config.data_input.header,
        csv_style=config.data_input.csv_style,
        predicted_table=config.data_input.predicted_table,
        max_imported_length=config.data_input.max_imported_length,
        request_datatype_suffix=config.data_input.request_datatype_suffix,
        data_orientation=config.data_input.data_orientation,
    )

    ###############
    ### ANCHOR ### Data consolidation
    #############

    if not config.data_input.predicted_column:
        config.data_input.predicted_column = 0

    data_for_predictions_df = mdp.preprocessing.data_consolidation(
        data,
        predicted_column=config.data_input.predicted_column,
        other_columns=config.data_input.other_columns,
        datalength=config.data_input.datalength,
        datetime_column=config.data_input.datetime_column,
        unique_threshold=config.data_input.unique_threshold,
        embedding=config.data_input.embedding,
        resample_freq=config.data_input.freq,
        resample_function=config.data_input.resample_function,
        remove_nans_threshold=config.data_input.remove_nans_threshold,
        remove_nans_or_replace=config.data_input.remove_nans_or_replace,
        dtype=config.data_input.dtype,
    )

    # In data consolidation predicted column was replaced on index 0 as first column
    predicted_column_index = 0
    predicted_column_name = data_for_predictions_df.columns[0]

    ###############
    ### ANCHOR ### Analyze original data
    #############

    column_for_predictions_series = data_for_predictions_df.iloc[:, 0:1]
    results = {}
    data_inputs = []

    if config.prediction.mode == "validate":
        column_for_predictions_series = column_for_predictions_series.iloc[: -config.output.predicts, :]
        config.prediction.repeatit = 1

    for i in config.models.used_models:
        data_inputs.append(config.models.models_input[i])
    data_inputs = set(data_inputs)

    if config.general.analyzeit == 1 or config.general.analyzeit == 3:
        print("Analyze of unprocessed data")
        try:
            predictit.analyze.analyze_column(data_for_predictions_df.values[:, 0], window=30)
            predictit.analyze.analyze_data(data_for_predictions_df)
            if config.general.analyze_seasonal_decompose:
                predictit.analyze.decompose(
                    data_for_predictions_df.values[:, 0],
                    **config.general.analyze_seasonal_decompose,
                )
        except Exception:
            mylogging.traceback("Analyze failed", level="ERROR")

    semaphor = None

    if config.general.multiprocessing:

        multiprocessing.freeze_support()

        if not config.general.processes_limit:
            config.general.processes_limit = multiprocessing.cpu_count() - 1

        if config.multiprocessing == "process":
            pipes = []
            semaphor = multiprocessing.Semaphore(config.general.processes_limit)

        elif config.multiprocessing == "pool":
            pool = multiprocessing.Pool(config.general.processes_limit)

            # It is not possible easy share data in multiprocessing, so results are resulted via callback function
            def return_result(result):
                for i, j in result.items():
                    results[i] = j

    time_table.add_entry("Data loading and preprocessing")
    update_gui("Predict", "progress_phase")

    ###############
    ### ANCHOR ### Feature extraction
    #############

    if config.feature_engineering.add_fft_columns:
        data_for_predictions_df = mdp.feature_engineering.add_frequency_columns(
            data_for_predictions_df,
            window=config.feature_engineering.add_fft_columns,
        )

    if config.feature_engineering.data_extension:
        data_for_predictions_df = mdp.feature_engineering.add_derived_columns(
            data_for_predictions_df, **config.feature_engineering.data_extension
        )

        ###############
        ### ANCHOR ### Feature selection
        #############

        # data_for_predictions_df TODO

        ###############
        ### ANCHOR ### Data preprocessing
        #############

    if config.prediction.mode == "validate":
        test_unstandardized = mdp.misc.split(data_for_predictions_df, predicts=config.output.predicts)[
            1
        ].values
        models_test_outputs_unstandardized = [test_unstandardized]

    else:
        test_unstandardized = None
        models_test_outputs_unstandardized = mdp.create_model_inputs.create_tests_outputs(
            data_for_predictions_df.values[:, 0],
            predicts=config.output.predicts,
            repeatit=config.prediction.repeatit,
        )

    data_for_predictions, last_undiff_value, final_scaler = mdp.preprocessing.preprocess_data(
        data_for_predictions_df.values,
        remove_outliers=config.data_input.remove_outliers,
        smoothit=config.data_input.smoothit,
        correlation_threshold=config.feature_engineering.correlation_threshold,
        data_transform=config.data_input.data_transform,
        standardizeit=config.data_input.standardizeit,
        bins=config.data_input.bins,
        binning_type=config.data_input.binning_type,
    )

    data_for_predictions = cast(np.ndarray, data_for_predictions)

    if config.prediction.mode == "validate":
        data_for_predictions, test = mdp.misc.split(data_for_predictions, predicts=config.predicts)
        models_test_outputs = [test]

    else:
        models_test_outputs = mdp.create_model_inputs.create_tests_outputs(
            data_for_predictions[:, 0],
            predicts=config.output.predicts,
            repeatit=config.prediction.repeatit,
        )

    column_for_predictions_processed = data_for_predictions[:, predicted_column_index]

    data_shape = np.shape(data_for_predictions)
    data_length = len(column_for_predictions_processed)

    data_std = np.std(column_for_predictions_processed[-30:])
    data_mean = np.mean(column_for_predictions_processed[-30:])
    data_abs_max = max(
        abs(column_for_predictions_processed.min()),
        abs(column_for_predictions_processed.max()),
    )

    multicolumn = 0 if data_shape[1] == 1 else 1

    if config.general.analyzeit in [2, 3]:

        print("\n\nAnalyze of preprocessed data\n")
        try:
            predictit.analyze.analyze_column(column_for_predictions_processed, window=30)
            predictit.analyze.analyze_data(data_for_predictions)
            if config.general.analyze_seasonal_decompose:
                predictit.analyze.decompose(
                    column_for_predictions_processed,
                    **config.general.analyze_seasonal_decompose,
                )

        except Exception:
            mylogging.traceback("Analyze failed", level="ERROR")

    min_data_length = 3 * config.output.predicts + config.data_input.default_n_steps_in

    if (
        data_length < min_data_length
        or data_length
        < config.prediction.repeatit + config.data_input.default_n_steps_in + config.output.predicts
    ):
        config.repeatit = 1
        min_data_length = 3 * config.output.predicts + config.data_input.default_n_steps_in

    assert min_data_length < data_length, mylogging.return_str(
        "Set up less predicted values in settings or add more data",
        caption="To few data",
    )

    for data_inputs_name in data_inputs:
        try:
            (
                model_train_input,
                model_predict_input,
                model_test_inputs,
            ) = mdp.create_model_inputs.create_inputs(
                data_for_predictions,
                input_type_name=data_inputs_name,
                input_type_params=config.models.data_inputs[data_inputs_name],
                mode=config.prediction.mode,
                predicts=config.output.predicts,
                repeatit=config.prediction.repeatit,
                predicted_column_index=predicted_column_index,
            )

        except Exception:
            mylogging.traceback(
                f"Error in creating input type: {data_inputs_name} with option optimization: {config_optimization_value}",
                level="WARNING",
            )
            continue

        for iterated_model_name in config.models.used_models:
            iterated_model = predictit.models.models_assignment[iterated_model_name]

            if config.models.models_input[iterated_model_name] == data_inputs_name:

                predict_parameters = {
                    "config": config.get_dict(),
                    # Functions to not import all modules
                    "preprocess_data_inverse": mdp.preprocessing.preprocess_data_inverse,
                    "fitted_power_transform": mdp.preprocessing.fitted_power_transform,
                    # Other
                    "iterated_model_train": iterated_model.train,
                    "iterated_model_predict": iterated_model.predict,
                    "iterated_model_name": iterated_model_name,
                    "model_train_input": model_train_input,
                    "model_predict_input": model_predict_input,
                    "model_test_inputs": model_test_inputs,
                    "models_test_outputs": models_test_outputs,
                    "models_test_outputs_unstandardized": models_test_outputs_unstandardized,
                    "data_abs_max": data_abs_max,
                    "data_mean": data_mean,
                    "data_std": data_std,
                    "last_undiff_value": last_undiff_value,
                    "final_scaler": final_scaler,
                    "semaphor": semaphor,
                }

                if config.models.models_input[iterated_model_name] in [
                    "one_step",
                    "one_step_constant",
                ]:
                    if multicolumn and config.output.predicts > 1:
                        mylogging.warn(
                            f"Warning in model {iterated_model_name} \n\nOne-step prediction on "
                            "multivariate data (more columns). Use multi_step (y length equals to predict) "
                            "or do use some one column data input in config models_input or predict just one value."
                        )
                        continue

                if config.general.multiprocessing == "process":

                    pipes.append(multiprocessing.Pipe(duplex=False))
                    p = multiprocessing.Process(
                        target=predictit._main_loop.train_and_predict,
                        kwargs={**predict_parameters, **{"pipe": pipes[-1][1]}},
                    )

                    p.Daemon = True  # Baby process will be terminated if parent killed
                    p.start()

                elif config.general.multiprocessing == "pool":

                    pool.apply_async(
                        predictit._main_loop.train_and_predict,
                        (),
                        predict_parameters,
                        callback=return_result,
                    )

                else:
                    results = {
                        **results,
                        **predictit._main_loop.train_and_predict(**predict_parameters),
                    }

    if config.general.multiprocessing:
        if config.general.multiprocessing == "process":
            for i in pipes:
                try:
                    results = {**results, **i[0].recv()}
                except Exception:
                    pass

        if config.general.multiprocessing == "pool":
            pool.close()
            pool.join()

        for i in results.values():
            mylogging.my_logger.log_and_warn_from_lists(i.logs_list, i.warnings_list)

    # Create confidence intervals
    if config.confidence_interval:
        try:
            lower_bound, upper_bound = predictit.misc.confidence_interval(
                column_for_predictions_series.values,
                predicts=config.output.predicts,
                confidence=config.prediction.confidence_interval,
            )

            grey_area = ["Lower bound", "Upper bound"]
            bounds = True
        except Exception:
            bounds = False
            grey_area = ["Lower bound", "Upper bound"]
            mylogging.traceback("Error in compute confidence interval", level="ERROR")

    else:
        bounds = False
        grey_area = False

    ###############
    ### ANCHOR ### Results processing
    #############

    # Criterion is the best of average from repetitions
    time_table.add_entry("Predict")
    update_gui("Evaluation", "progress_phase")

    # results_df contain info around prediction. Model errors, time, memory peak etc.
    results_df = pd.DataFrame.from_dict(results, orient="index")

    if results_df.empty:
        raise RuntimeError(
            mylogging.return_str(
                "None of models finished predictions. Set config.output.logger_subconfig.logger_level = 'DEBUG' for more info.",
                caption="All models failed for some reason",
            )
        )

    if config.output.sort_results_by == "model":
        results_df.sort_index(key=lambda x: x.str.lower(), inplace=True)

    elif config.output.sort_results_by == "error":
        results_df.sort_values("model_error", inplace=True)

    best_model_name = results_df["model_error"].idxmin()

    # Generate date indexes for result predictions
    last_date = column_for_predictions_series.index[-1]

    if isinstance(
        last_date,
        (pd.core.indexes.datetimes.DatetimeIndex, pd._libs.tslibs.timestamps.Timestamp),
    ):
        date_index = pd.date_range(
            start=last_date,
            periods=config.output.predicts + 1,
            freq=column_for_predictions_series.index.freq,
        )[1:]
        date_index = pd.to_datetime(date_index)

    else:
        date_index = pd.Index(list(range(last_date + 1, last_date + config.output.predicts + 1)))

    predictions_df = pd.DataFrame(
        results_df["prediction"].values.tolist(), columns=date_index, index=results_df.index
    ).T

    best_model_predictions = predictions_df[best_model_name]

    if predictions_df.empty:
        raise RuntimeError(
            mylogging.return_str(
                "Neither of models finished prediction. Set config.output.logger_subconfig.logger_level = 'DEBUG' for more info."
            )
        )

    if config.hyperparameter_optimization.optimizeit:
        hyperparameter_optimization = results_df["Hyperparameter optimization"].to_dict()
    else:
        hyperparameter_optimization = None

    not_flat_columns = [
        i
        for i in [
            "hyperparameter_optimization",
            "logs_list",
            "model",
            "test_errors",
            "trained_model",
            "warnings_list",
        ]
        if i in results_df.columns
    ]

    details_df = results_df[not_flat_columns]
    results_df.drop(columns=not_flat_columns, inplace=True)

    results_df.rename(
        columns={
            "memory_peak_MB": "Memory peak MB",
            "model_error": "Model error",
            "model_time": "Model time",
            "model": "Model",
            "prediction": "Prediction",
            "unstandardized_model_error": "Unstandardized model error",
        },
        inplace=True,
    )

    # If validating, bold black line is test 'real' values.
    # If predict mode, black line is the best prediction
    if config.prediction.mode == "validate":
        predictions_df.insert(0, "Test", test_unstandardized)

    ###############
    ### ANCHOR ### Plot
    #############

    predictions_for_plot = predictions_df.copy()

    predictions_for_plot.columns = [f"{i + 1} - {j}" for i, j in enumerate(predictions_for_plot.columns)]

    best_model_name_plot = "Test" if config.prediction.mode == "validate" else predictions_for_plot.columns[0]

    bounds_df = pd.DataFrame(index=date_index)

    if bounds:
        bounds_df["Upper bound"] = upper_bound
        bounds_df["Lower bound"] = lower_bound

    last_value = float(column_for_predictions_series.iloc[-1, 0])

    predictions_for_plot_limited = pd.concat(
        [predictions_for_plot.iloc[:, : config.output.plot_subconfig.plot_number_of_models], bounds_df],
        axis=1,
    )

    history = column_for_predictions_series[-config.output.plot_subconfig.plot_history_length :]

    predictions_with_history = pd.concat(
        [
            history,
            predictions_for_plot_limited,
        ],
        sort=False,
    )
    predictions_with_history.iloc[-config.output.predicts - 1, :] = last_value

    if config.general.analyzeit:
        import matplotlib.pyplot as plt

        plt.show()

    time_table.add_entry("Evaluation")
    update_gui("plot", "progress_phase")

    if config.output.plot_subconfig.show_plot or config.output.plot_subconfig.save_plot:

        with warnings.catch_warnings():
            warnings.simplefilter("ignore", ResourceWarning)

            return_div = True if _GUI else False

            div = mypythontools.plots.plot(
                predictions_with_history,
                plot_library=config.output.plot_subconfig.plot_library,
                title=config.output.plot_subconfig.plot_title,
                legend=config.output.plot_subconfig.plot_legend,
                highlighted_column=predicted_column_name,
                surrounded_column=best_model_name_plot,
                grey_area=grey_area,
                save=config.output.plot_subconfig.save_plot,
                return_div=return_div,
                show=config.output.plot_subconfig.show_plot,
            )

    time_table.add_entry("plot")
    time_table.finish_table(table_format=config.output.table_settings)
    update_gui("Completed", "progress_phase")

    ###############
    ### ANCHOR ### Table
    #############

    simple_table_df = mdp.misc.edit_table_to_printable(
        results_df[["Model error"]]
        .iloc[: config.output.print_subconfig.print_number_of_models, :]
        .reset_index()
    )

    detailed_table_df = results_df.iloc[
        : config.output.print_subconfig.print_number_of_models, :
    ].reset_index()
    detailed_table_df.drop(["Unstandardized model error"], axis=1, inplace=True)
    detailed_table_df = mdp.misc.edit_table_to_printable(detailed_table_df)

    tables = predictit.result_classes.Tables(
        simple=tabulate(
            simple_table_df.values,
            headers=["Model", f"Average {config.prediction.error_criterion} error"],
            **config.output.table_settings,
        ),
        detailed=tabulate(
            detailed_table_df.values,
            headers=list(detailed_table_df.columns),
            **config.output.table_settings,
        ),
        time=time_table.time_table,
        simple_table_df=simple_table_df,
        detailed_table_df=detailed_table_df,
        time_df=time_table.time_df,
    )

    ###############
    ### ANCHOR ### Results
    #############

    result_details = predictit.result_classes.ResultDetails(
        prediction_index=date_index, history=history, last_value=last_value, test=test_unstandardized
    )

    result = predictit.result_classes.Result(
        best_prediction=best_model_predictions,
        best_model_name=best_model_name,
        predictions=predictions_df,
        results_df=results_df,
        details_df=details_df,
        results=results,
        with_history=predictions_with_history,
        tables=tables,
        config=config,
        details=result_details,
        hyperparameter_optimization=hyperparameter_optimization,
    )

    ###############
    ### ANCHOR ### Print
    #############

    if config.output.print_subconfig.print:
        if config.output.print_subconfig.print_result_details:
            print(
                (
                    f"\nBest model is {best_model_name} with results \n\n{best_model_predictions}\n\nWith model error {config.error_criterion} = "
                    f"{results_df.loc[best_model_name, 'Model error']}"
                )
            )

        if config.output.print_subconfig.print_table == "simple":
            print(f"\n{tables.simple}\n")

        elif config.output.print_subconfig.print_table == "detailed":
            print(f"\n{tables.detailed}\n")

        if config.output.print_subconfig.print_time_table:
            print(f"\n{tables.time}\n")

    ###############
    ### ANCHOR ### Return
    #############

    mylogging.reset_outer_warnings_filter()

    # Return stdout and stop collect warnings and printed output
    if _GUI:
        output = sys.stdout.getvalue()
        sys.stdout = stdout
        result.output = output
        print(output)
        result.plot = div

    if config.general.keepinternal_results:
        result.internal_result = {
            "data_for_predictions (X, y)": data_for_predictions,
            "model_train_input": model_train_input,
            "model_predict_input": model_predict_input,
            "model_test_inputs": model_test_inputs,
            "models_test_outputs": models_test_outputs,
        }

    return result


def predict_multiple_columns(
    data=None,
    predicted_columns: list | tuple | str | None = None,
    freqs: list | tuple | str | None = None,
    config: predictit.configuration.Config | dict | None = None,
    **kwargs,
) -> predictit.result_classes.Multiple:
    """Predict multiple columns and multiple frequencies at once. Use predict function.

    Only data and predicted_columns can be positional.

    Check README or tests for working examples.

    Args:
        data (np.ndarray, pd.DataFrame): Time series. Can be 2-D - more columns.
            !!! In Numpy array use data series as rows, but in dataframe use cols !!!. Defaults to [].
        predicted_columns (list | tuple | str | None, optional): List of indexes of predicted columns or it's names (dataframe).
            Defaults to None.
        freqs (list | tuple | str | None, optional): If date index available, resample data and predict in defined
            time frequency. If None, then value from config will be used. Defaults to [].
        config (predictit.configuration.Config | dict | None, optional): Settings as Config instance or dictionary.
            Check class for what you can use. If None, then default config will be used. Defaults to None.
        **kwargs (dict, optional): There is much more parameters' in this function. Check configuration.py
            for parameters details.

    Returns:
        np.ndarray: All the predicted results.
    """
    config = predictit._helpers.parse_config(config, config_default, kwargs)

    # Edit configuration.py default values with arguments values if exist
    if data is not None:
        config.data_input.data = data

    if predicted_columns is not None:
        config.data_input.predicted_columns = predicted_columns

    if freqs is not None:
        config.data_input.freqs = freqs

    if not config.data_input.predicted_columns or not isinstance(config.data_input.predicted_columns, list):
        raise TypeError(
            mylogging.return_str("predict_multiple function need predicted_columns config value to be list.")
        )

    predictit._helpers.logger_init_from_config(config.output.logger_subconfig)

    if not config.data_input.freqs:
        freqs = ["Default frequency"]
    else:
        freqs = config.data_input.freqs

    if config.data_input.predicted_columns in ["*", ["*"]]:

        if isinstance(config.data_input.data, str):
            data = mdp.load_data.load_data(
                config.data_input.data,
                header=config.data_input.header,
                csv_style=config.data_input.csv_style,
                predicted_table=config.data_input.predicted_table,
                max_imported_length=config.data_input.max_imported_length,
                request_datatype_suffix=config.data_input.request_datatype_suffix,
                data_orientation=config.data_input.data_orientation,
            )

        config.data_input.predicted_columns = mdp.preprocessing.data_consolidation(data).columns

    results = {}
    best_predictions_dataframes = {}

    # TODO check indexes
    for fi, f in enumerate(freqs):

        result_dataframe = pd.DataFrame()

        for ci, c in enumerate(config.data_input.predicted_columns):

            config.data_input.predicted_column = c
            config.data_input.freq = f

            result_name = f"Column: {c}" if len(freqs) == 1 else f"Column: {c} - Freq: {f}"

            try:
                results[result_name] = predict(config=config)

                result_dataframe[c] = results[result_name].best_prediction

            except Exception:
                mylogging.traceback(
                    f"Error in making predictions on column {c} and freq {f}",
                    level="ERROR",
                )

        best_predictions_dataframes[f"Freq: {f}"] = result_dataframe

    return predictit.result_classes.Multiple(
        best_predictions_dataframes=best_predictions_dataframes, results=results
    )


# def compare_models(
#     data_all=None,
#     predicted_column: list | tuple | str | None = None,
#     config: predictit.configuration.Config | dict | None = None,
#     **kwargs,
# ) -> predictit.result_classes.Comparison:
#     """Function that helps to choose appropriate models. It evaluates it on test data and then return results.
#     After you know what models are the best, you can use only them in functions predict() or predict_multiple_columns.
#     You can define your own test data and find best modules for your process.

#     Only data_all and predicted column can be positional.

#     Check README or tests for working examples.

#     Args:
#         data_all ((dict, None)): Dictionary of data name as key and config data field and used column as value
#             `{data_1: (my_dataframe, 'column_name_or_index')}` or tuple of data with same predicted
#             column configured in config `(my_data[-2000:], my_data[-1000:])`
#         predicted_column (list | tuple | str | None, optional): Index of predicted column or it's name (dataframe).
#             If list with more values only the first one will be evaluated (use predict_multiple_columns function
#             if you need that. Default to None.
#         config (predictit.configuration.Config | dict | None, optional): Settings as Config instance or dictionary.
#             Check class for what you can use. If None, then default config will be used. Defaults to None.
#         **kwargs (dict, optional): There is much more parameters' in this function. Check configuration.py for parameters details.
#     """

#     from tabulate import tabulate

#     config = predictit._helpers.parse_config(config, config_default, kwargs)

#     # Edit configuration.py default values with arguments values if exist
#     if data_all is not None:
#         config.data_input.data_all = data_all

#     if predicted_column is not None:
#         config.data_input.predicted_column = predicted_column

#     predictit._helpers.logger_init_from_config(config.output.logger_subconfig)

#     # Edit config.py default values with arguments values if exist
#     config.update(
#         {
#             "mode": "validate",
#             "confidence_interval": None,
#             "optimizeit": False,
#             "evaluate_type": "preprocessed",
#             "print_result_details": False,
#             "print_time_table": False,
#         }
#     )

#     # If no data_all inserted, default will be used
#     if not config.data_input.data_all:
#         config.data_input.data_all = {
#             "sin": (mdp.generate_data.sin(), 0),
#             "Sign": (mdp.generate_data.sign(), 0),
#             "Random data": (mdp.generate_data.random(), 0),
#         }
#         mylogging.warn("Test data was used. Setup 'data_all' in config...")

#     data_dict = config.data_input.data_all
#     same_data = False

#     if isinstance(data_dict, (list, tuple, np.ndarray)):
#         same_data = True
#         data_dict = {f"Data {i}": (j, config.data_input.predicted_column) for i, j in enumerate(data_dict)}

#     # TODO FIX compare models where optimizing more models...
#     optimization_number = (
#         len(config.config_optimization.optimization) if config.config_optimization.optimization else 1
#     )

#     results_errors_absolute_array = np.zeros(
#         (len(data_dict), optimization_number, len(config.models.used_models))
#     )
#     results_errors_absolute_array.fill(np.nan)
#     results_errors_standardized_array = results_errors_absolute_array.copy()

#     all_models_results = {}

#     for g, (i, j) in enumerate(data_dict.items()):

#         config.data_input.data = j[0]
#         if not same_data:
#             config.data_input.predicted_column = j[1]

#         config.output.plot_subconfig.plot_title = i

#         try:

#             prediction_result = predict(config=config)
#             all_models_results[i] = j
#             evaluated_matrix = prediction_result.misc.evaluated_matrix
#             results_errors_absolute_array[g] = evaluated_matrix

#             # Standardize results to be able to have average error through different data
#             if np.nanmax(evaluated_matrix) - np.nanmin(evaluated_matrix) > 0:
#                 results_errors_standardized_array[g] = (evaluated_matrix - np.nanmin(evaluated_matrix)) / (
#                     np.nanmax(evaluated_matrix) - np.nanmin(evaluated_matrix)
#                 )
#             else:
#                 results_errors_standardized_array[g] = evaluated_matrix

#         except Exception:
#             mylogging.traceback(f"Comparison for data {i} didn't finished.", level="ERROR")

#     (
#         best_results_errors,
#         best_values_dict,
#         values_results,
#         best_model_name,
#         best_value,
#     ) = predictit.analyze.analyze_results(
#         results_errors_absolute_array,
#         config.config_optimization.optimization_values,
#         config.models.used_models,
#         config.prediction.error_criterion,
#     )

#     (
#         best_results_errors_on_standardized_data,
#         best_models_optimization_values_on_standardized_data_dict,
#         optimization_values_results_on_standardized_data_df,
#         best_model_name_on_standardized_data,
#         best_value_on_standardized_data,
#     ) = predictit.analyze.analyze_results(
#         results_errors_standardized_array,
#         config.config_optimization.optimization_values,
#         config.models.used_models,
#         config.prediction.error_criterion,
#     )

#     evaluated_results_df = pd.DataFrame(index=config.models.used_models)
#     evaluated_results_df["Errors average"] = best_results_errors
#     evaluated_results_df["Standardized\nerror average"] = best_results_errors_on_standardized_data

#     if config.config_optimization.optimization:
#         evaluated_results_df["Best optimized\nvalues"] = best_values_dict.values()
#         evaluated_results_df[
#             "Standardized best\noptimized values"
#         ] = best_models_optimization_values_on_standardized_data_dict.values()

#     if config.output.sort_results_by == "error":
#         evaluated_results_df.sort_values("Errors average", inplace=True)

#     if config.output.print_subconfig_compare_models.print_table:
#         print(
#             "\n\nComplete results for comparison"
#             "\nTable of complete results. Percentual standardized error is between 0 and 1. If 0, model was the "
#             "best on all defined data, 1 means it was the worst."
#         )

#     simple_table_df = mdp.misc.edit_table_to_printable(
#         evaluated_results_df.drop(
#             [
#                 i
#                 for i in [
#                     "Standardized\nerror average",
#                     "Standardized best\noptimized values",
#                 ]
#                 if i in evaluated_results_df.columns
#             ],
#             axis=1,
#         )
#         .iloc[: config.output.print_subconfig.print_number_of_comparison_models, :]
#         .reset_index()
#     )
#     detailed_table_df = (
#         mdp.misc.edit_table_to_printable(evaluated_results_df)
#         .iloc[: config.output.print_subconfig.print_number_of_comparison_models, :]
#         .reset_index()
#     )
#     tables = predictit.result_classes.Tables(
#         simple=tabulate(
#             simple_table_df.values,
#             headers=simple_table_df.columns,
#             **config.output.table_settings,
#         ),
#         detailed=tabulate(
#             detailed_table_df.values,
#             headers=detailed_table_df.columns,
#             **config.output.table_settings,
#         ),
#         time="TODO To be implemented",
#         simple_table_df=simple_table_df,
#         detailed_table_df=detailed_table_df,
#         time_df="TODO To be implemented",
#     )

#     if config.output.print_subconfig_compare_models.print_comparison_result_details:

#         if best_model_name == best_model_name_on_standardized_data:
#             print(f"Best model is {best_model_name}\n")
#         else:
#             print(
#                 f"Best model on average error is {best_model_name} and best of standardized errors is {best_model_name_on_standardized_data}\n"
#             )

#         if config.config_optimization.optimization:
#             if best_value == best_value_on_standardized_data:
#                 print(
#                     f"Best optimized value for {config.config_optimization.optimization_variables} is {best_value}\n"
#                 )
#             else:
#                 print(f"Best optimized value on absolute error is {best_value}")
#                 print(f"Best optimized value on standardized error is {best_value_on_standardized_data}\n")

#     if config.output.print_subconfig_compare_models.print_comparison_table == "simple":
#         print(f"\n{tables.simple}\n")

#     elif config.output.print_subconfig_compare_models.print_comparison_table == "detailed":
#         print(f"\n{tables.detailed}\n")

#     # if config.output.print_subconfig.print_time_table:
#     # TODO
#     # print(f"\n{tables.time}\n")

#     if config.config_optimization.optimization:

#         # TODO change optimization result for all variables
#         config_optimization_result = predictit.result_classes.ConfigOptimization(
#             optimization_variable=config.config_optimization.optimization_variables,
#             optimization_values=config.config_optimization.optimization_values,
#             best_value=best_model_name,
#             values_results=values_results,
#             best_values_dict=best_values_dict,
#         )

#         optimization_standardized_result = predictit.result_classes.hyperparameter_optimization(
#             optimization_variable=config.config_optimization.optimization_variables,
#             optimization_values=config.config_optimization.optimization_values,
#             best_value=best_model_name_on_standardized_data,
#             values_results=optimization_values_results_on_standardized_data_df,
#             best_values_dict=best_models_optimization_values_on_standardized_data_dict,
#         )
#     else:
#         config_optimization_result = None
#         optimization_standardized_result = None

#     standardized_result = predictit.result_classes.ComparisonStandardized(
#         best_model_name_standardized=best_model_name_on_standardized_data,
#         best_value_standardized=best_value_on_standardized_data,
#         optimization_standardized=optimization_standardized_result,
#     )
#     comparison_result = predictit.result_classes.Comparison(
#         results_df=evaluated_results_df,
#         best_model_name=best_model_name,
#         best_value=best_value,
#         tables=tables,
#         all_models_results=all_models_results,
#         standardized_results=standardized_result,
#         optimization=config_optimization_result,
#     )

#     return comparison_result


if __name__ == "__main__":

    parser_args_dict = None

    if len(sys.argv) > 1 and not GLOBAL_VARS.JUPYTER:

        # Add settings from command line if used
        parser = argparse.ArgumentParser(
            usage=(
                "\n\nPrediction framework configuration via command line parser!"
                "There is many configurable variables that you can use. For possible "
                "options check configuration.py where all the variables are described.\n\n"
                "Use as usual. Open terminal in predictit folder and run for example: \n\n"
                'python predictit/main.py --predicts 30 --plot_title "My fancy plot"'
            ),
        )

        config_dict = config_default.get_dict()

        for i in config_dict.keys():
            # TODO add help
            parser.add_argument(f"--{i}")

        # Non empty command line args
        parser_args_dict = {
            k: mypythontools.misc.str_to_infer_type(v)
            for k, v in parser.parse_known_args()[0].__dict__.items()
            if v is not None
        }

        config_dict.update(parser_args_dict)

    if config_default.general.used_function == "predict":
        prediction_results = predict(config=parser_args_dict)

    elif config_default.general.used_function == "predict_multiple":
        prediction_results = predict_multiple_columns(config=parser_args_dict)
