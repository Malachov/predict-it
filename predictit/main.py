#!/usr/bin/python
#%%
""" This is main module for making predictions. After setup config['py'] (data source), run predict function.
There is also predict_multiple_columns function for predicting more columns. It's not necessary to run all models everytime,
so there is also function compare models, that return results of models on the test data. Then you can config again only good models.

Do not edit this file if you are user, it's not necassary! Only call function from here. Only file to edit is config.py. If you are developer, edit as you need.

There are working examples in main readme and also in test_it module. Particular modules functionality is vissible in visual.py in tests.
"""
import sys
from pathlib import Path
import numpy as np
from prettytable import PrettyTable
import time
import os
# import plotly as pl - lazy load later
import pandas as pd
import warnings
import traceback
import argparse
import inspect


this_path = Path(__file__).resolve().parents[1]
this_path_string = str(this_path)

if 'predictit' not in sys.modules:

    # If used not as a library but as standalone framework, add path to be able to import predictit if not opened in folder
    sys.path.insert(0, this_path_string)

    import predictit

from predictit.config import config, presets
import predictit.data_prep as dp
from misc import traceback_warning, _GUI


try:
    import gui_start  # Not included in predictit if used as python library
except Exception:
    pass


def update_gui(content, id):
    try:
        gui_start.edit_gui_py(content, id)
    except Exception:
        pass

if __name__ == "__main__":

    # All the config is in config.py" - rest only for people that know what are they doing
    # Add settings from command line if used
    parser = argparse.ArgumentParser(description='Prediction framework setting via command line parser!')
    parser.add_argument("--use_config_preset", type=str, choices=[None, 'fast'], help="Edit some selected config, other remains the same, check config_presets.py file.")
    parser.add_argument("--used_function", type=str, choices=['predict', 'predict_multiple_columns', 'compare_models'], help="Which function in main.py use. One predict one column, ohter more at once, the last compare models on test data")
    parser.add_argument("--data_source", type=str, choices=['csv', 'test'], help="What source of data to use")
    parser.add_argument("--csv_full_path", type=str, help="Full CSV path with suffix")
    parser.add_argument("--predicts", type=int, help="Number of predicted values - 7 by default")
    parser.add_argument("--predicted_column", type=int, help="Name of predicted column or it's index - string or int")
    parser.add_argument("--predicted_columns", type=list, help="For predict_multiple_columns function only! List of names of predicted column or it's indexes")
    parser.add_argument("--freq", type=str, help="Interval for predictions 'M' - months, 'D' - Days, 'H' - Hours")
    parser.add_argument("--freqs", type=list, help="For predict_multiple_columns function only! List of intervals of predictions 'M' - months, 'D' - Days, 'H' - Hours")
    parser.add_argument("--plot", type=bool, help="If 1, plot interactive graph")
    parser.add_argument("--date_index", type=int, help="Index of dataframe or it's name. Can be empty, then index 0 or new if no date column.")
    parser.add_argument("--return_type", type=str, choices=['best', 'all', 'dict', 'model_criterion'], help="What will be returned by function. Best result, all results (array), dict with more results (best, all, string of plot in HTML...) or model_criterion with MAPE or RMSE (based on config) of all models")
    parser.add_argument("--datalength", type=int, help="The length of the data used for prediction")
    parser.add_argument("--debug", type=bool, help="Debug - print all results and all the errors on the way")
    parser.add_argument("--analyzeit", type=bool, help="Analyze input data - Statistical distribution, autocorrelation, seasonal decomposition etc.")
    parser.add_argument("--optimizeit", type=bool, help="Find optimal parameters of models")
    parser.add_argument("--repeatit", type=int, help="How many times is computation repeated")
    parser.add_argument("--other_columns", type=bool, help="If 0, only predicted column will be used for making predictions.")
    parser.add_argument("--lengths", type=bool, help="Compute on various length of data (1, 1/2, 1/4...). Automatically choose the best length. If 0, use only full length.")
    parser.add_argument("--remove_outliers", type=bool, help="Remove extraordinary values. Value is threshold for ignored values. Value means how many times standard deviation from the average threshold is far")
    parser.add_argument("--standardize", type=str, choices=[None, 'standardize', '-11', '01', 'robust'], help="Data standardization, so all columns have similiar scopes")
    parser.add_argument("--criterion", type=str, choices=['mape', 'rmse'], help="Error criterion used for model")
    parser.add_argument("--compareit", type=int, help="How many models will be displayed in final plot. 0 if only the best one.")

    # Non empty command line args
    parser_args_dict = {k: v for k, v in parser.parse_known_args()[0].__dict__.items() if v is not None}

    # Edit config.py default values with command line arguments values if exist
    for i, j in parser_args_dict.items():

        if i in config:

            config[i] = j
        else:
            warnings.warn(f"\n \t Inserted option with command line --{i} not found in config.py use --help for more information.\n")

if config['debug'] == 1:
    warnings.filterwarnings('once')
elif config['debug'] == 2:
    warnings.filterwarnings('error')
else:
    warnings.filterwarnings('ignore')

for i in config['ignored_warnings']:
    warnings.filterwarnings('ignore', message=fr"[\s\S]*{i}*")


def predict(data=None, predicts=None, predicted_column=None, freq=None, models_parameters=None, data_source=None,
            csv_full_path=None, plot=None, used_models=None, date_index=None, return_type=None, datalength=None, data_transform=None, debug=None, analyzeit=None,
            optimizeit=None, repeatit=None, other_columns=None, lengths=None, remove_outliers=None, standardize=None, criterion=None, compareit=None, n_steps_in=None, output_shape=None):

    """Make predictions mostly on time-series data. Data input can be set up in config.py (E.G. csv type, path and name),
    data can be used as input arguments to function (this has higher priority) or it can be setup as command line arguments (highest priority).
    Values defaults are None, because are in config.py

    Args:
        data (np.ndarray, pd.DataFrame): Time series. Can be 2-D - more columns.
            !!! In Numpy array use data series as rows, but in dataframe use cols !!!. If you use CSV, leave it empty. Defaults to [].
        predicts (int, optional): Number of predicted values. Defaults to None.
        predicted_column (int, str, optional): Index of predicted column or it's name (dataframe).
            If list with more values only the first one will be evaluated (use predict_multiple_columns function if you need that. Defaults to None.
        freq (str. 'H' or 'D' or 'M', optional): If date index available, resample data and predict in defined time frequency. Defaults to None.

    Returns:
        np.ndarray: Evaluated predicted values.
        If in setup - return all models results {np.ndarray}.
        If in setup - return interactive plot of results.

    """


    # Add everything printed + warnings to variable to be able to print in GUI
    if _GUI:
        import io

        stdout = sys.stdout
        sys.stdout = io.StringIO()

    if config["use_config_preset"] and config["use_config_preset"] != 'none':
        config.update(presets[config["use_config_preset"]])

    # Parse all functions parameters and it's values to edit config.py later
    frame = inspect.currentframe()
    args, _, _, values = inspect.getargvalues(frame)

    # # Edit config.py default values with arguments values if exist
    for i in args:

        if values[i] is not None:
            if i in config:
                config[i] = values[i]

            else:
                warnings.warn(f"\n \t Inserted option with function argument --{i} not found in config.py.\n")

    # Do not repeat actually mean evaluate once
    if not config['repeatit']:
        config['repeatit'] = 1

    # Definition of the table for spent time on code parts
    time_parts_table = PrettyTable()
    time_parts_table.field_names = ["Part", "Time"]

    def update_time_table(time_last):
        time_parts_table.add_row([progress_phase, time.time() - time_last])
        return time.time()
    time_point = time_begin = time.time()

    ##########################################
    ################## DATA ########### ANCHOR Data
    ##########################################

    progress_phase = "Data loading and preprocessing"
    update_gui(progress_phase, 'progress_phase')

    if config['data'] is None:

        ############# Load CSV data #############
        if config['data_source'] == 'csv':
            if config['csv_test_data_relative_path']:
                try:
                    data_location = this_path / 'predictit' / 'test_data'
                    csv_path = data_location / config['csv_test_data_relative_path']
                    config['csv_full_path'] = Path(csv_path).as_posix()
                except Exception:
                    print(f"\n ERROR - Test data load failed - Setup CSV adress and column name in config \n\n")
                    raise
            try:
                config['data'] = pd.read_csv(config['csv_full_path'], header=0).iloc[-config['datalength']:, :]
            except Exception:
                print("\n ERROR - Data load failed - Setup CSV adress and column name in config \n\n")
                raise

        ############# Load SQL data #############
        elif config['data_source'] == 'sql':
            try:
                config['data'] = predictit.database.database_load(server=config['server'], database=config['database'], freq=config['freq'], data_limit=config['datalength'], last=config['last_row'])
            except Exception:
                print("\n ERROR - Data load from SQL server failed - Setup server, database and predicted column name in config \n\n")
                raise

        elif config['data_source'] == 'test':
            config['data'] = predictit.test_data.generate_test_data.gen_random(config['datalength'])

    ### pd.Series ###

    if isinstance(config['data'], pd.Series):

        predicted_column_index = 0
        predicted_column_name = 'Predicted Column'

        data_for_predictions_df = pd.DataFrame(data[-config['datalength']:])

        if config['remove_outliers']:
            data_for_predictions_df = dp.remove_outliers(data_for_predictions_df, predicted_column_index=predicted_column_index, threshold=config['remove_outliers'])

        data_for_predictions = data_for_predictions_df.values

    ### pd.DataFrame ###

    elif isinstance(config['data'], pd.DataFrame):
        data_for_predictions_df = config['data'].iloc[-config['datalength']:, ]

        if isinstance(config['predicted_column'], str):

            predicted_column_name = config['predicted_column']
            predicted_column_index = data_for_predictions_df.columns.get_loc(predicted_column_name)
        else:
            predicted_column_index = config['predicted_column']
            predicted_column_name = data_for_predictions_df.columns[predicted_column_index]

        if config['date_index']:

            if isinstance(config['date_index'], str):
                data_for_predictions_df.set_index(config['date_index'], drop=True, inplace=True)
            else:
                data_for_predictions_df.set_index(data_for_predictions_df.columns[config['date_index']], drop=True, inplace=True)

            data_for_predictions_df.index = pd.to_datetime(data_for_predictions_df.index)

            if config['freq']:
                data_for_predictions_df.sort_index(inplace=True)
                data_for_predictions_df.resample(config['freq']).sum()

            else:
                config['freq'] = data_for_predictions_df.index.freq

                if config['freq'] is None:
                    data_for_predictions_df.reset_index(inplace=True)

        # Make predicted column index 0
        data_for_predictions_df.insert(0, predicted_column_name, data_for_predictions_df.pop(predicted_column_name))
        predicted_column_index = 0

        if config['other_columns']:

            data_for_predictions_df = dp.remove_nan_columns(data_for_predictions_df)

            if config['remove_outliers']:
                data_for_predictions_df = dp.remove_outliers(data_for_predictions_df, predicted_column_index=predicted_column_index, threshold=config['remove_outliers'])

            data_for_predictions_df = dp.keep_corelated_data(data_for_predictions_df)

            data_for_predictions = data_for_predictions_df.values.T

        else:
            if config['remove_outliers']:
                data_for_predictions_df = dp.remove_outliers(data_for_predictions_df, predicted_column_index=predicted_column_index, threshold=config['remove_outliers'])

            data_for_predictions = data_for_predictions_df[predicted_column_name].to_frame().values.T

    ### np.ndarray ###

    elif isinstance(config['data'], np.ndarray):
        data_for_predictions = config['data']
        predicted_column_name = 'Predicted column'

        if len(np.shape(config['data'])) > 1 and np.shape(config['data'])[0] != 1:
            if np.shape(config['data'])[1] > np.shape(config['data'])[0]:
                data_for_predictions = data_for_predictions.T
            data_for_predictions = data_for_predictions[:, -config['datalength']:]

            if config['other_columns']:
                # Make predicted column on index 0
                data_for_predictions[[0, config['predicted_column']], :] = data_for_predictions[[config['predicted_column'], 0], :]

                predicted_column_index = 0
                if config['remove_outliers']:
                    data_for_predictions = dp.remove_outliers(data_for_predictions, predicted_column_index=predicted_column_index, threshold=config['remove_outliers'])

                data_for_predictions = dp.keep_corelated_data(data_for_predictions)
            else:
                data_for_predictions = data_for_predictions[predicted_column_index]
                predicted_column_index = 0
                if config['remove_outliers']:
                    data_for_predictions = dp.remove_outliers(data_for_predictions, threshold=config['remove_outliers'])

            data_for_predictions_df = pd.DataFrame(data_for_predictions, columns=[predicted_column_name])

        else:
            data_for_predictions = data_for_predictions[-config['datalength']:].reshape(1, -1)

            predicted_column_index = 0
            if config['remove_outliers']:
                data_for_predictions = dp.remove_outliers(data_for_predictions, predicted_column_index=predicted_column_index, threshold=config['remove_outliers'])

            data_for_predictions_df = pd.DataFrame(data_for_predictions.reshape(-1), columns=[predicted_column_name])

    ### Data preprocessing, common for all datatypes ###

    data_for_predictions = data_for_predictions.astype(config['dtype'], copy=False)

    data_shape = data_for_predictions.shape

    column_for_prediction_dataframe = data_for_predictions_df[data_for_predictions_df.columns[0]].to_frame()
    column_for_plot = column_for_prediction_dataframe.iloc[-7 * config['predicts']:]

    last_value = column_for_plot.iloc[-1]
    try:
        number_check = int(last_value)

    except Exception:
        print(f"\n ERROR - Predicting not a number datatype. Maybe bad config['predicted_columns'] setup.\n Predicted datatype is {type(column_for_prediction[1])} \n\n")
        raise

    if config['data_transform'] == 'difference':

        for i in range(len(data_for_predictions)):
            data_for_predictions[i, 1:] = dp.do_difference(data_for_predictions[i])

        data_for_predictions = np.delete(data_for_predictions, 0, axis=1)

    if config['standardize'] == '01':
        data_for_predictions, final_scaler = dp.standardize(data_for_predictions, standardizer=config['standardize'])
    if config['standardize'] == '-11':
        data_for_predictions, final_scaler = dp.standardize(data_for_predictions, standardizer=config['standardize'])
    if config['standardize'] == 'standardize':
        data_for_predictions, final_scaler = dp.standardize(data_for_predictions, standardizer=config['standardize'])
    if config['standardize'] == 'robust':
        data_for_predictions, final_scaler = dp.standardize(data_for_predictions, standardizer=config['standardize'])

    if data_for_predictions.ndim == 1:
        column_for_prediction = data_for_predictions
    else:
        column_for_prediction = data_for_predictions[predicted_column_index]

    ###################################
    ############# Analyze ###### ANCHOR Analyze
    ###################################

    data_shape = np.shape(data_for_predictions)
    data_length = len(column_for_prediction)

    data_std = np.std(data_for_predictions[0, -30:])
    data_mean = np.mean(data_for_predictions[0, -30:])
    data_abs_max = max(abs(column_for_prediction.min()), abs(column_for_prediction.max()))

    if data_for_predictions.ndim == 1 or (data_for_predictions.ndim == 2 and (data_shape[0] == 1 or data_shape[1] == 1)):
        multicolumn = 0
    else:
        multicolumn = 1

    if config['analyzeit']:
        predictit.analyze.analyze_data(data_for_predictions.T, window=30)

        # TODO repair decompose
        #predictit.analyze.decompose(column_for_prediction_dataframe, freq=36, model='multiplicative')

    min_data_length = 3 * config['predicts'] + config['repeatit'] * config['predicts'] + config['default_n_steps_in']

    if data_length < min_data_length:
        config['repeatit'] = 1
        min_data_length = 3 * config['predicts'] + config['repeatit'] * config['predicts'] + config['default_n_steps_in']

    assert (min_data_length < data_length), 'To few data - set up less repeat value in settings or add more data'

    if config['lengths']:
        data_lengths = [data_length, int(data_length / 2), int(data_length / 4), min_data_length + 50, min_data_length]
        data_lengths = [k for k in data_lengths if k >= min_data_length]
    else:
        data_lengths = [data_length]

    data_number = len(data_lengths)

    models_names = list(config['used_models'].keys())
    models_number = len(models_names)
    params_everywhere = {"predicts": config['predicts']}

    ##########################################
    ################ Optimize ################ ANCHOR Optimize
    ##########################################

    for i in models_names:

        # If no parameters or parameters details, add it so no index errors later
        if i not in config['models_parameters']:
            config['models_parameters'][i] = {}

    if config['optimizeit']:

        time_point = update_time_table(time_point)
        progress_phase = "Optimizing"
        update_gui(progress_phase, 'progress_phase')

        models_optimizations_time = config['used_models'].copy()

        ### Define training inputs
        train, test = dp.split(data_for_predictions, predicts=config['predicts'], predicted_column_index=predicted_column_index)
        used_default_inputs_optimize, used_derived_inputs_optimize = predictit.define_inputs.find_used_inputs(models_names)

        #####################################
        ######### Optimize loop ###### ANCHOR Inputs for optimize
        #####################################

        for default_input in used_default_inputs_optimize:
            used_sequentions = predictit.define_inputs.create_inputs(default_input, used_derived_inputs_optimize, train, predicted_column_index=predicted_column_index, multicolumn=multicolumn)

            for iterated_model_index, (iterated_model_name, iterated_model) in enumerate(config['used_models'].items()):
                if config['models_input'][iterated_model_name] in used_sequentions.keys() and iterated_model_name in config['models_parameters_limits']:

                    try:
                        start_optimization = time.time()
                        model_kwargs = {**config['models_parameters'][iterated_model_name], **params_everywhere}

                        best_kwargs = predictit.best_params.optimize(iterated_model, model_kwargs, config['models_parameters_limits'][iterated_model_name], test=test, train_input=used_sequentions[config['models_input'][iterated_model_name]], fragments=config['fragments'], iterations=config['iterations'], time_limit=config['optimizeit_limit'], criterion=config['criterion'], name=iterated_model_name, details=config['optimizeit_details'])

                        for k, l in best_kwargs.items():

                            config['models_parameters'][iterated_model_name][k] = l

                    except Exception:
                        if config['debug']:
                            traceback_warning("Optimization didn't finished")

                    finally:
                        stop_optimization = time.time()
                        models_optimizations_time[i] = (stop_optimization - start_optimization)

    # Empty boxes for results definition
    # The final result is - [repeated, model, data, results]
    test_results_matrix = np.zeros((config['repeatit'], models_number, data_number, config['predicts']))
    evaluated_matrix = np.zeros((config['repeatit'], models_number, data_number))
    test_results_matrix.fill(np.nan)
    evaluated_matrix.fill(np.nan)

    reality_results_matrix = np.zeros((models_number, data_number, config['predicts']))

    models_time = {}

    data_end = None

    time_point = update_time_table(time_point)
    progress_phase = "Predict"
    update_gui(progress_phase, 'progress_phase')

    trained_models = {}

    test_sequentions = np.zeros((config['repeatit'], config['predicts']))

    test_end = -config['predicts']
    for i in range(config['repeatit']):
        test_sequentions[i] = column_for_prediction[test_end - config['predicts']: test_end]
        test_end -= 1

    test_sequentions = test_sequentions[::-1]


    # Repeat evaluation on shifted data to eliminate randomness
    for data_length_index, data_length_iteration in enumerate(data_lengths):

        ### Define training inputs
        #train, test = dp.split(data_for_predictions[:, :data_end], predicts=config['predicts'], predicted_column_index=predicted_column_index)

        if data_end:
            data_start = data_length - data_length_iteration + data_end
        else:
            data_start = data_length - data_length_iteration
        if data_start < 0:
            data_start = 0


        used_default_inputs, used_derived_inputs = predictit.define_inputs.find_used_inputs(models_names)

        for default_input in used_default_inputs:
            used_sequentions = predictit.define_inputs.create_inputs(default_input, used_derived_inputs, data_for_predictions, predicted_column_index=predicted_column_index, multicolumn=multicolumn)




            ################################################
            ############# Predict and evaluate ############# ANCHOR Predict
            ################################################

            for iterated_model_index, (iterated_model_name, iterated_model) in enumerate(config['used_models'].items()):
                if config['models_input'][iterated_model_name] in used_sequentions.keys():

                    this_sequention = used_sequentions[config['models_input'][iterated_model_name]]
                    if isinstance(this_sequention, tuple):
                        model_train_input = (this_sequention[0][data_start:, :], this_sequention[1][data_start:, :])
                        model_test_input = this_sequention[0][-config['predicts'] - config['repeatit']: -config['predicts'], :]
                        model_predict_input = this_sequention[2]

                    elif this_sequention.ndim == 1:
                        model_train_input = model_predict_input = this_sequention[data_start:]
                    else:
                        model_train_input = model_predict_input = this_sequention[:, data_start:]

                    try:
                        start = time.time()

                        trained_models[iterated_model_name] = iterated_model.train(model_train_input, predicted_column_index=0, **params_everywhere, **config['models_parameters'][iterated_model_name])

                        reality_results_matrix[iterated_model_index, data_length_index] = iterated_model.predict(model_predict_input, trained_models[iterated_model_name], predicted_column_index=0, **params_everywhere, **config['models_parameters'][iterated_model_name])

                        # Remove wrong values out of scope to not be plotted
                        reality_results_matrix[iterated_model_index, data_length_index][abs(reality_results_matrix[iterated_model_index, data_length_index]) > 10 * data_abs_max] = np.nan

                        if config['standardize']:
                            reality_results_matrix[iterated_model_index, data_length_index] = final_scaler.inverse_transform(reality_results_matrix[iterated_model_index, data_length_index])

                        if config['data_transform'] == 'difference':
                            reality_results_matrix[iterated_model_index, data_length_index] = dp.inverse_difference(reality_results_matrix[iterated_model_index, data_length_index], last_value)

                        for repeat_iteration in range(config['repeatit']):

                            test_results_matrix[repeat_iteration, iterated_model_index, data_length_index] = iterated_model.predict(model_test_input[repeat_iteration], trained_models[iterated_model_name], predicted_column_index=0, **params_everywhere, **config['models_parameters'][iterated_model_name])

                            if config['power_transformed'] == 2:
                                test_results_matrix[repeat_iteration, iterated_model_index, data_length_index] = dp.fitted_power_transform(test_results_matrix[repeat_iteration, iterated_model_index, data_length_index], data_std, data_mean)

                            evaluated_matrix[repeat_iteration, iterated_model_index, data_length_index] = predictit.evaluate_predictions.compare_predicted_to_test(test_results_matrix[repeat_iteration, iterated_model_index, data_length_index], test_sequentions[repeat_iteration], criterion=config['criterion'])

                    except Exception:

                        if config['debug']:
                            traceback_warning(f"Error in train {iterated_model_name} model on data length {data_length_iteration}")

                    finally:
                        models_time[iterated_model_name] = (time.time() - start)

    # Criterion is the best of average from repetitions
    time_point = update_time_table(time_point)
    progress_phase = "Evaluation"
    update_gui(progress_phase, 'progress_phase')

    repeated_average = np.mean(evaluated_matrix, axis=0)

    model_results = []

    for i in repeated_average:
        model_results.append(np.nan if np.isnan(i).all() else np.nanmin(i))

    sorted_results = np.argsort(model_results)

    if config['compareit']:
        sorted_results = sorted_results[:config['compareit']]
    else:
        sorted_results = sorted_results[0]

    predicted_models = {}

    for i, j in enumerate(sorted_results):
        this_model = list(config['used_models'].keys())[j]

        if i == 0:
            best_model_name = this_model


        predicted_models[this_model] = {'order': i, 'criterion': model_results[j], 'predictions': reality_results_matrix[j, np.argmin(repeated_average[j])], 'data_length': np.argmin(repeated_average[j])}

    ##########################################
    ############# Results ############# ANCHOR Table
    ##########################################

    best_model_predicts = predicted_models[best_model_name]['predictions']

    if config['print_result']:
        print(f"\n Best model is {best_model_name} \n\t with results {best_model_predicts} \n\t with model error {config['criterion']} = {predicted_models[best_model_name]['criterion']} \n\t with data length {data_lengths[predicted_models[best_model_name]['data_length']]} \n\t with paramters {config['models_parameters'][best_model_name]} \n")


    # Definition of the table for results
    models_table = PrettyTable()
    models_table.field_names = ['Model', f"Average {config['criterion']} error", 'Train time', 'Predict time']

    # Fill the table
    for i, j in predicted_models.items():
        models_table.add_row([i, j['criterion'], models_time[i]])


    if config['print_table']:
        print(f'\n {models_table} \n')

    ### Print detailed resuts ###

    if config['debug']:

        for i, j in enumerate(models_names):
            print(models_names[i])

            for k in range(data_number):
                print(f"\t With data length: {data_lengths[k]}  {config['criterion']} = {repeated_average[i, k]} \n")

            if config['optimizeit']:
                print(f"\t Time to optimize {models_optimizations_time[j]} \n")
                print("Best models parameters", config['models_parameters'][j])

    ###############################
    ######### Plot ######### ANCHOR Results
    ###############################
    time_point = update_time_table(time_point)
    progress_phase = "plot"
    update_gui(progress_phase, 'progress_phase')

    if config['plot']:

        plot_return = 'div' if _GUI else ''
        div = predictit.plot.plotit(column_for_plot, predicted_models, plot_type=config['plot_type'], show=config['show_plot'], save=config['save_plot'], save_path=config['save_plot_path'], plot_return=plot_return)

    time_point = update_time_table(time_point)
    progress_phase = 'finished'
    update_gui(progress_phase, 'progress_phase')
    time_parts_table.add_row(['Complete time', time.time() - time_begin])

    if config['print_time_table']:
        print(f'\n {time_parts_table} \n')

    # Return stdout and stop collect warnings and printed output
    if _GUI:
        output = sys.stdout.getvalue()
        sys.stdout = stdout

    if config['return_type'] == 'model_criterion':
        return repeated_average

    if config['return_type'] == 'all':
        return results

    elif not config['return_type'] or config['return_type'] == 'best':
        return best_model_predicts

    elif config['return_type'] == 'dict':
        results = {}
        results['best'] = best_model_predicts
        results['all'] = results
        results['time_table'] = time_parts_table.get_html_string()
        results['models_table'] = models_table.get_html_string()
        if _GUI:
            results['plot'] = div
            results['output'] = output

        return results


def predict_multiple_columns(data=None, predicted_columns=None, freqs=None, database_deploy=None, predicts=None, models_parameters=None, data_source=None,
            csv_full_path=None, plot=None, used_models=None, date_index=None, return_all=None, datalength=None, data_transform=None, debug=None, analyzeit=None,
            optimizeit=None, repeatit=None, other_columns=None, lengths=None, remove_outliers=None, standardize=None, criterion=None, compareit=None, n_steps_in=None, output_shape=None):
    """Predict multiple colums and multiple frequencions at once. Use predict function.

    Args:
        data (np.ndarray, pd.DataFrame): Time series. Can be 2-D - more columns.
            !!! In Numpy array use data series as rows, but in dataframe use cols !!!. Defaults to [].
        predicted_columns (list, optional): List of indexes of predicted columns or it's names (dataframe). Defaults to None.
        freqs (str. 'H' or 'D' or 'M', optional): If date index available, resample data and predict in defined time frequency. Defaults to [].
        database_deploy (bool, optional): Whether deploy results to database !!!
            For every database it's necessary to adjust the database function. Defaults to 0.

    Returns:
        np.ndarray: All the predicted results.
    """

    # Parse all functions parameters and it's values to edit config['py'] later
    frame = inspect.currentframe()
    args, _, _, values = inspect.getargvalues(frame)

    # Edit config.py default values with arguments values if exist
    for i in args:
        if values[i] is not None and i in config:
            config[i] = values[i]
        else:
            warnings.warn(f"\n \t Inserted option with command line --{i} not found in config.py use --help for more information.\n")

    predictions_full = [np.nan] * len(config['freqs'])

    for j in range(len(config['freqs'])):

        predictions = [np.nan] * len(config['predicted_columns'])

        for i in range(len(config['predicted_columns'])):

            try:
                predictions[i] = predict(predicted_column=config['predicted_columns'][i], freq=config['freqs'][j])

            except Exception:
                traceback_warning(f"Error in making predictions on column {config['predicted_columns'][i]} and freq {config['freqs'][j]}")

        predictions_full[j] = predictions

        # # TODO last_date resolve
        # if config['database_deploy']:
        #     try:
        #         predictit.database.database_deploy(config["server"], config["database"], last_date, predictions[0], predictions[1], freq=config['freqs'][j])
        #     except Exception:
        #         traceback_warning(f"Error in database deploying on freq {j}")

    return predictions_full


def compare_models(data_all=None, predicts=None, predicted_column=None, freq=None, models_parameters=None, data_source=None,
            csv_full_path=None, plot=None, used_models=None, date_index=None, return_all=None, datalength=None, data_transform=None, debug=None, analyzeit=None,
            optimizeit=None, repeatit=None, other_columns=None, lengths=None, remove_outliers=None, standardize=None, criterion=None, compareit=None, n_steps_in=None, output_shape=None):
    """Function that helps to choose apropriate models. It evaluate it on test data and then return results.
    After you know what models are the best, you can use only them in functions predict() or predict_multiple_columns.
    You can define your own test data and find best modules for your process. You can pickle data if you are use it
    more often to faster loading.

    Args:
        data_all (dict): Dictionary of data names and data values (np.array). You can use data from test_data module, generate_test_data script (e.g. gen_sin()).
        data_length (int, optional): If data are pickled, length that will be used. Defaults to 1000.
        **kwargs (dict): Data specific parameters. Mostly for predicted_column value.

    """

    # Parse all functions parameters and it's values to edit config.py later
    frame = inspect.currentframe()
    args, _, _, values = inspect.getargvalues(frame)

    # # Edit config.py default values with arguments values if exist
    for i in args:

        if values[i] is not None:
            if i in config:
                config[i] = values[i]

            else:
                warnings.warn(f"\n \t Inserted option with function argument --{i} not found in config.py.\n")

    config['lengths'] = 1
    config['repeatit'] = 3

    # If no data_all inserted, default will be used
    if config['data_all'] is None:
        config['data_all'] = {'sin': predictit.test_data.generate_test_data.gen_sin(config['datalength']), 'Sign': predictit.test_data.generate_test_data.gen_sign(config['datalength']), 'Random data': predictit.test_data.generate_test_data.gen_random(config['datalength'])}

    ### Pickle option was removed, add if you need it...
    # if config['pickleit']:
    #     from predictit.test_data.pickle_test_data import pickle_data_all
    #     import pickle
    #     pickle_data_all(config['data_all'], datalength=config['datalength'])

    # if config['from_pickled']:

    #     script_dir = Path(__file__).resolve().parent
    #     data_folder = script_dir / "test_data" / "pickled"

    #     for i, j in config['data_all'].items():
    #         file_name = i + '.pickle'
    #         file_path = data_folder / file_name
    #         try:
    #             with open(file_path, "rb") as input_file:
    #                 config['data_all'][i] = pickle.load(input_file)
    #         except Exception:
    #             traceback_warning(f"Test data not loaded - First in config['py'] pickleit = 1, that save the data on disk, then load from pickled.")

    results = {}

    for i, j in config['data_all'].items():
        config['plot_name'] = i
        try:
            result = predictit.main.predict(data=j, return_type="model_criterion")

            results[i] = (result - np.nanmin(result)) / (np.nanmax(result) - np.nanmin(result))

        except Exception:
            traceback_warning(f"Comparison for data {i} didn't finished.")
            results[i] = np.nan

    results_array = np.stack(list(results.values()), axis=0)

    all_data_average = np.nanmean(results_array, axis=0)

    models_best_results = []

    for i in all_data_average:
        models_best_results.append(np.nan if np.isnan(i).all() else np.nanmin(i))
    models_best_results = np.array(models_best_results)

    best_compared_model = int(np.nanargmin(models_best_results))
    best_compared_model_name = list(config['used_models'].keys())[best_compared_model]

    all_lengths_average = np.nanmean(all_data_average, axis=0)
    best_all_lengths_index = np.nanargmin(all_lengths_average)

    print("\n\nTable of complete results. Percentual standardized error is between 0 and 1. If 0, model was the best on all defined data, 1 means it was the worst.")
    models_table = PrettyTable()
    models_table.field_names = ['Model', f'Percentual standardized error']

    # Fill the table
    for i, j in enumerate(config['used_models']):
        models_table.add_row([j, models_best_results[i]])

    print(f'\n {models_table} \n')

    print(f"\n\nBest model is {best_compared_model_name}")
    print(f"\n\nBest data length index is {best_all_lengths_index}")


if __name__ == "__main__" and config['used_function']:
    if config['used_function'] == 'predict':
        results = predict()

    elif config['used_function'] == 'predict_multiple_columns':
        results = predict_multiple_columns()

    elif config['used_function'] == 'compare_models':
        compare_models()
